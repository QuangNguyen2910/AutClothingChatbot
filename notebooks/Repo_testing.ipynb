{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Clone Repo and Install necessary library"
      ],
      "metadata": {
        "id": "ATo-vObNlz8c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MHAoTO9elQje"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/QuangNguyen2910/AutClothingChatbot.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd AutClothingChatbot"
      ],
      "metadata": {
        "id": "g_9p9wtHlYmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install -r requirements.txt\n",
        "!pip install gradio"
      ],
      "metadata": {
        "id": "r-UKDJ9Flb12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check GPU Resources"
      ],
      "metadata": {
        "id": "jmt82M04loAv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/AutClothingChatbot\n",
        "!python test.py"
      ],
      "metadata": {
        "id": "2bYSadW2lnUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## For some reason I didn't figure it out yet, running the **`Initiate Chatbot with file run`** will slower the chatbot x2 or even more so if you want to use it for faster inference, for now, just run the **`Initiate Chatbot with code run`**"
      ],
      "metadata": {
        "id": "cMy7F4penW7u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initiate Chatbot with file run"
      ],
      "metadata": {
        "id": "4Rughpc4mKEY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This kernel will be use when you need to know what need to put in for running the chatbot\n",
        "!python main.py --help"
      ],
      "metadata": {
        "id": "gCpMAKUOmUit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py -mn \"Quangnguyen711/clothes_shop_chatbot_QLoRA\" \\\n",
        "-l4 \"True\" -hf \"\" -ms \"2048\" -dt \"None\" \\\n",
        "-emn \"thenlper/gte-small\" -d \"deploy\""
      ],
      "metadata": {
        "id": "MTMmqgNgmjjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initiate Chatbot with code run"
      ],
      "metadata": {
        "id": "PFlIrVjRmvVT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from core import ChatbotModel, RagAgent, Finetuner\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores.utils import DistanceStrategy\n",
        "from unsloth import FastLanguageModel\n",
        "import gradio as gr\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, StoppingCriteria, StoppingCriteriaList, TextIteratorStreamer\n",
        "from threading import Thread\n",
        "\n",
        "def get_prompt_template(tokenizer):\n",
        "    question_prompt = \"\"\"\n",
        "    ### Question:\n",
        "    {}\n",
        "    ### Contexts:\n",
        "    {}\n",
        "    ### Answer:\n",
        "    \"\"\"\n",
        "    chat = [\n",
        "        {\"role\": \"system\", \"content\": \"Your name is Aut, you are a helpful and friendly clothing consultant. Your job is to help the customers if they need any help with our website using or any clothing suggestion. Answer the question base on the given contexts below.\"},\n",
        "        {\"role\": \"user\", \"content\": question_prompt},\n",
        "        {\"role\": \"assistant\", \"content\": \"{}\"}\n",
        "    ]\n",
        "\n",
        "    formated_prompt = tokenizer.apply_chat_template(chat, tokenize=False)\n",
        "\n",
        "    return formated_prompt\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "# Fix the parameter in here if you want to use another model or change methods or variables\n",
        "#---------------------------------------------------------------------------------------------------------------------------------\n",
        "    MODEL_NAME = \"Quangnguyen711/clothes_shop_chatbot_QLoRA\"\n",
        "    LOAD_IN_4BIT = \"True\"\n",
        "    HF_TOKEN = \"\"\n",
        "    MAX_SEQ_LENGTH = \"2048\"\n",
        "    DTYPE = None\n",
        "    EMBEDDING_MODEL_NAME = \"thenlper/gte-small\"\n",
        "    DISPlAY = \"deploy\"\n",
        "#---------------------------------------------------------------------------------------------------------------------------------\n",
        "    LOAD_IN_4BIT = True if LOAD_IN_4BIT == \"True\" else False\n",
        "    HF_TOKEN = str(HF_TOKEN)\n",
        "    MAX_SEQ_LENGTH = 2048 if MAX_SEQ_LENGTH in [None, \"None\"] else int(MAX_SEQ_LENGTH)\n",
        "    DTYPE = None if DTYPE in [None, \"None\"] else eval(DTYPE)\n",
        "    EMBEDDING_MODEL_NAME = \"thenlper/gte-small\" if EMBEDDING_MODEL_NAME in [None, \"None\"] else EMBEDDING_MODEL_NAME\n",
        "    DISPlAY = \"kernel\" if DISPlAY in [None, \"None\"] else DISPlAY\n",
        "\n",
        "    model, tokenizer = ChatbotModel(MODEL_NAME, HF_TOKEN, LOAD_IN_4BIT, MAX_SEQ_LENGTH, DTYPE).get_pretrained_model()\n",
        "    formated_prompt = Finetuner.get_prompt_template(tokenizer)\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    print('------------------------------------------------------------------------------------------------------------')\n",
        "    print('Model loaded successfully!')\n",
        "    print('------------------------------------------------------------------------------------------------------------')\n",
        "\n",
        "    embedding_model = HuggingFaceEmbeddings(\n",
        "        model_name=EMBEDDING_MODEL_NAME,\n",
        "        multi_process=True,\n",
        "        model_kwargs={\"device\": \"cuda\"},\n",
        "        encode_kwargs={\"normalize_embeddings\": True},  # Set `True` for cosine similarity\n",
        "    )\n",
        "\n",
        "    files = [\"docs/Autumn_RAG.txt\",]\n",
        "\n",
        "    knowledge_base = RagAgent.document_chunking(files)\n",
        "\n",
        "    KNOWLEDGE_VECTOR_DATABASE = FAISS.from_documents(\n",
        "        knowledge_base, embedding_model, distance_strategy=DistanceStrategy.COSINE\n",
        "    )\n",
        "\n",
        "    print('------------------------------------------------------------------------------------------------------------')\n",
        "    print('Knowledge base loaded successfully!')\n",
        "\n",
        "    system_command = \"\"\"\n",
        "    Your name is Aut, you are a really helpful and friendly clothing consultant.\n",
        "    Your job is to help the customers if they need any help with our website using or any clothing suggestion.\n",
        "    Answer the question base on the given contexts below if there is one.\n",
        "    Do not answer questions that are not related to our clothes shop.\n",
        "    If the answer cannot be deduced from the context, do not give an answer.\n",
        "    \"\"\".strip()\n",
        "\n",
        "    question_prompt = \"\"\"\n",
        "    ### Question:\n",
        "    {}\n",
        "    ### Contexts:\n",
        "    {}\n",
        "    ### Answer:\n",
        "    \"\"\".strip()\n",
        "\n",
        "    if DISPlAY == \"kernel\":\n",
        "        while(True):\n",
        "\n",
        "            test_question = input(\"Enter the question: \")\n",
        "            retrieved_docs = KNOWLEDGE_VECTOR_DATABASE.similarity_search(query=test_question, k=1, fetch_k=4)\n",
        "            test_context = retrieved_docs[0].page_content.replace(\"**\", \"\")\n",
        "\n",
        "            FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "            messages = [\n",
        "                {\"role\": \"system\", \"content\": system_command},\n",
        "                {\"role\": \"user\", \"content\": question_prompt.format(test_question, test_context)},\n",
        "            ]\n",
        "\n",
        "            prompt = tokenizer.apply_chat_template(messages, tokenize = False)\n",
        "\n",
        "            inputs = tokenizer.apply_chat_template(\n",
        "                messages,\n",
        "                tokenize = True,\n",
        "                add_generation_prompt = True, # Must add for generation\n",
        "                return_dict = True,\n",
        "                return_tensors = \"pt\",\n",
        "            ).to(\"cuda\")\n",
        "\n",
        "            outputs = model.generate(input_ids = inputs.input_ids, max_new_tokens = 128, use_cache = True)\n",
        "\n",
        "            print('------------------------------------------------------------------------------------------------------------')\n",
        "            print(\"Aut: \", tokenizer.batch_decode(outputs[:, inputs.input_ids.shape[1]:], skip_special_tokens=True)[0])\n",
        "    elif DISPlAY == \"api\":\n",
        "        pass\n",
        "    else:\n",
        "        FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "        class StopOnTokens(StoppingCriteria):\n",
        "            def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
        "                stop_ids = [29, 0]\n",
        "                for stop_id in stop_ids:\n",
        "                    if input_ids[0][-1] == stop_id:\n",
        "                        return True\n",
        "                return False\n",
        "\n",
        "        def predict(message, history):\n",
        "            history_transformer_format = [{\"role\": \"system\", \"content\": system_command}]\n",
        "            retrieved_docs = KNOWLEDGE_VECTOR_DATABASE.similarity_search(query=message, k=1, fetch_k=4)\n",
        "            msg_context = retrieved_docs[0].page_content.replace(\"**\", \"\")\n",
        "            for human, assistant in history[1:]:\n",
        "                history_transformer_format.append({\"role\": \"user\", \"content\": human })\n",
        "                history_transformer_format.append({\"role\": \"assistant\", \"content\":assistant})\n",
        "            history_transformer_format.append({\"role\": \"user\", \"content\": question_prompt.format(message, msg_context)})\n",
        "            stop = StopOnTokens()\n",
        "\n",
        "            messages = history_transformer_format\n",
        "\n",
        "            model_inputs = tokenizer.apply_chat_template(\n",
        "                    [messages],\n",
        "                    tokenize = True,\n",
        "                    add_generation_prompt = True, # Must add for generation\n",
        "                    return_dict = True,\n",
        "                    return_tensors = \"pt\",\n",
        "            ).to(device)\n",
        "            streamer = TextIteratorStreamer(tokenizer, timeout=10., skip_prompt=True, skip_special_tokens=True)\n",
        "            generate_kwargs = dict(model_inputs, streamer=streamer, max_new_tokens=128)\n",
        "            t = Thread(target=model.generate, kwargs=generate_kwargs)\n",
        "            t.start()\n",
        "\n",
        "            partial_message = \"\"\n",
        "            for new_token in streamer:\n",
        "                if new_token != '<':\n",
        "                    partial_message += new_token\n",
        "                    yield partial_message\n",
        "\n",
        "        gr.ChatInterface(predict).launch(inline=False)\n"
      ],
      "metadata": {
        "id": "fhvjfBYxmzO2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}